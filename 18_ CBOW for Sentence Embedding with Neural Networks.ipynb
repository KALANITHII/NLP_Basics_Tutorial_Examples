{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"CBOW Implementation for Sentence Embedding Using Neural Networks\"\n",
    "\n",
    "This notebook presents a Python implementation of the Continuous Bag of Words (CBOW) model for sentence embedding using neural networks. CBOW is a popular word embedding technique that learns distributed representations of words in a sentence by predicting the target word based on its context. The implementation showcases the application of neural networks to efficiently capture semantic relationships within sentences, providing a foundation for understanding and utilizing CBOW in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert the text data to sequences of word indices\n",
    "sequences = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "# Define the context and target words\n",
    "context_size = 2\n",
    "word_pairs = []\n",
    "for i in range(context_size, len(sequences) - context_size):\n",
    "    context_words = sequences[i - context_size:i] + sequences[i + 1:i + context_size + 1]\n",
    "    target_word = sequences[i]\n",
    "    word_pairs.append((context_words, target_word))\n",
    "    \n",
    "# Split the data into training and validation sets\n",
    "split = int(0.8 * len(word_pairs))\n",
    "train_data = word_pairs[:split]\n",
    "val_data = word_pairs[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100, input_length=context_size*2),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "X_train = np.array([context_words for context_words, _ in train_data])\n",
    "y_train = np.array([target_word for _, target_word in train_data])\n",
    "X_val = np.array([context_words for context_words, _ in val_data])\n",
    "y_val = np.array([target_word for _, target_word in val_data])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'quick' are:\n",
      "jumps (0.48)\n",
      "brown (0.16)\n",
      "the (0.15)\n",
      "over (0.15)\n",
      "fox (0.12)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "word_embeddings = model.get_weights()[0]\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "def get_word_vector(word):\n",
    "    word_index = tokenizer.word_index[word]\n",
    "    return word_embeddings[word_index]\n",
    "\n",
    "test_word = \"quick\"\n",
    "test_word_vector = get_word_vector(test_word)\n",
    "most_similar_words = []\n",
    "for i in range(1, vocab_size):\n",
    "    if i != word_index[test_word]:\n",
    "        word = reverse_word_index[i]\n",
    "        word_vector = get_word_vector(word)\n",
    "        similarity = np.dot(test_word_vector, word_vector) / (np.linalg.norm(test_word_vector) * np.linalg.norm(word_vector))\n",
    "        most_similar_words.append((word, similarity))\n",
    "most_similar_words = sorted(most_similar_words, key=lambda x: x[1], reverse=True)\n",
    "print(\"Most similar words to '{}' are:\".format(test_word))\n",
    "for word, similarity in most_similar_words[:5]:\n",
    "    print(\"{} ({:.2f})\".format(word, similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
